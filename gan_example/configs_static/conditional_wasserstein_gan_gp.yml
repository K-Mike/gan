runner_params:
  # input keys
  # Note: for albumentations transforms we have to have key "image" =(
  data_input_key: &real_data "image"  # do not change!
  class_input_key: &class_targets "class_targets"
  noise_input_key: &noise_input "noise"
  # output keys
  fake_logits_output_key: &fake_validity "fake_validity"
  real_logits_output_key: &real_validity "real_validity"
  fake_data_output_key: &fake_data "fake_image"
  # condition keys (as this is conditional gan)
  fake_condition_keys: [&class_targets_one_hot "class_targets_one_hot"]
  real_condition_keys: ["class_targets_one_hot"]
  # phases
  generator_train_phase: &generator_train_phase generator_train
  discriminator_train_phase: &discriminator_train_phase discriminator_train
  # model keys:
  generator_model_key: &generator_model_name "generator"
  discriminator_model_key: &critic_model_name "critic"

common_hyperparams:
  dummy_input_key: &dummy_input_key "image"  # temporary workaround
  wd: &wasserstein_distance "wasserstein_distance"

model_params:
  _key_value: True
  generator:
    model: SimpleCGenerator
    noise_dim: &noise_dim 16
    num_classes: &n_classes 10
  critic:
    model: SimpleCDiscriminator
    num_classes: *n_classes
  feature_extractor:
    model: mnist
    pretrained: True


args:
  expdir: "gan_example"
  baselogdir: "./logs/gan_example/conditional_wasserstein_gan_gp/static"


stages:

  transform_params:
    transform: A.Compose
    transforms:
      - transform: AsImage
      - transform: A.Normalize
        mean: [0.5]
        std: [0.5]
      - transform: A.ToTensorV2
      - transform: AdditionalNoiseTensor
        tensor_size: [*noise_dim]
        output_key: *noise_input
      - transform: OneHotTargetTransform
        num_classes: *n_classes
        input_key: *class_targets
        output_key: *class_targets_one_hot

  data_params:
    batch_size: 64
    num_workers: 0

    datasets:
      train:
        dataset: MNIST
        train: True
      valid:
        dataset: MNIST
        train: False

    image_key: *real_data
    target_key: *class_targets

  state_params:
    num_epochs: 100
    main_metric: "metrics/FID"
    minimize_metric: True
    batch_consistant_metrics: False
    # todo: add somewhere
    memory: {}
    prev_batch_metrics: {}

  criterion_params:
    _key_value: True
    mean_output_loss:
      criterion: MeanOutputLoss
    gradient_penalty:
      criterion: GradientPenaltyLoss

  callbacks_params:
    phase_manager:
      callback: PhaseManagerCallback
      # one of "all" (use all callbacks), "same" (same phases as in train)
      valid_mode: "all"
      train_phases:
        *discriminator_train_phase: 5
        *generator_train_phase: 1

    tricky_metric_manager_callback: # saves batch_metrics to prev_batch_metrics
      callback: TrickyMetricManagerCallback

    loss_g:
      _wrapper: &g_train_wrapper
        callback: PhaseBatchWrapperCallback
        active_phases: [*generator_train_phase]
      callback: CriterionCallback
      input_key: *dummy_input_key  # input key does not matter
      output_key: *fake_validity
      criterion_key: mean_output_loss
      prefix: loss_g
      multiplier: -1.0

    loss_d_real:
      _wrapper: &d_train_wrapper
        callback: PhaseBatchWrapperCallback
        active_phases: [*discriminator_train_phase]
      callback: CriterionCallback
      input_key: *dummy_input_key  # input key does not matter
      output_key: *real_validity
      criterion_key: mean_output_loss
      prefix: loss_d_real
    loss_d_fake:
      _wrapper: *d_train_wrapper
      callback: CriterionCallback
      input_key: *dummy_input_key  # input key does not matter
      output_key: *fake_validity
      criterion_key: mean_output_loss
      prefix: loss_d_fake
    loss_d_gp:
      _wrapper: *d_train_wrapper
      callback: GradientPenaltyCallback
      real_input_key: *real_data
      fake_output_key: *fake_data
      critic_model_key: *critic_model_name
      condition_keys: [*class_targets_one_hot] # None
      criterion_key: gradient_penalty
      prefix: "loss_d_gp"
    loss_d:
      _wrapper: *d_train_wrapper
      callback: MetricAggregationCallback
      mode: "weighted_sum"
      prefix: &loss_d loss_d
      metrics:
        loss_d_real: -1.0
        loss_d_fake: 1.0
        loss_d_gp: 10.0  # gradient penalty multiplier

    optim_g:
      _wrapper: *g_train_wrapper
      callback: OptimizerCallback
      optimizer_key: generator
      loss_key: loss_g
    optim_d:
      _wrapper: *d_train_wrapper
      callback: OptimizerCallback
      optimizer_key: discriminator
      loss_key: loss_d

    wasserstein_distance:
#      _wrapper: *d_train_wrapper
      callback: WassersteinDistanceCallback
      prefix: *wasserstein_distance
      real_validity_output_key: *real_validity
      fake_validity_output_key: *fake_validity

    viz:
      callback: ConstNoiseVisualizerCalback
      noise_dim: *noise_dim
      n_classes: *n_classes

    saver:
      callback: CheckpointCallback

    perceptual_path_length:
      callback: CGanPerceptualPathLengthCallback
      prefix: "metrics/PPL"
      generator_model_key: *generator_model_name
      embedder_model_key: feature_extractor
      noise_shape: *noise_dim
      num_samples: 100  # debug [recommended real value ~10-100k]
      eps: 1e-2  # debug

    # Metric-related callbacks:
    # TODO (important; low priority; usability) - how to compress this 100 lines to a reasonable amount (ideally single callback)?

    memorizer:
      callback: MemoryAccumulatorCallback
      input_key:
        *real_data: &memory_real_data "real_data"
      output_key:
        *fake_data: &memory_fake_data "fake_data"
      memory_size: 200

    feature_extractor_real:
      callback: MemoryFeatureExtractorCallback
      memory_key: *memory_real_data
      model_key: "feature_extractor"
      channels: 1
      layer_key:
        model.fc2: &memory_real_features "real_features"
#        layer2: &memory_real_features "real_features"
        "":
          activation:
            name: "softmax"
            dim: -1
          memory_out_key: &memory_real_probabilities "real_probabilities"

    feature_extractor_fake:
      callback: MemoryFeatureExtractorCallback
      memory_key: *memory_fake_data
      model_key: "feature_extractor"
      channels: 1
      layer_key:
        model.fc2: &memory_fake_features "fake_features"
#        layer2: &memory_fake_features "fake_features"
        "":
          activation:
            name: "softmax"
            dim: -1
          memory_out_key: &memory_fake_probabilities "fake_probabilities"

    distance_real_real_px:
      callback: MemoryTransformCallback
      batch_transform: RealFakeDistanceBatchTransform
      transform_in_key:
        *memory_real_data: X_real
        *memory_fake_data: X_fake
      transform_out_key: "D_px"
      suffixes: ["rr", "rf", "ff"]

    # scores on raw pixel data

    knn_scores:
      callback: MemoryMultiMetricCallback
      prefix: "metrics/knn"
      suffixes: ["acc", "acc_real", "acc_fake"]
      metric: KnnScores
      memory_key:
        D_px_rr: D_XX
        D_px_rf: D_XY
        D_px_ff: D_YY
      # metric_kwargs:
      k: 1

    frechet_inception_distance_px:
      callback: MemoryMetricCallback
      memory_key:
        *memory_fake_data: samples_a
        *memory_real_data: samples_b
      prefix: "metrics/FID_px"
      metric: "FrechetInceptionDistance"

    # scores on conv feature data

    inception_score_conv:
      callback: MemoryMetricCallback
      memory_key:
        *memory_fake_probabilities: samples
      prefix: "metrics/IS"
      metric: "InceptionScore"

    mode_score_conv:
      callback: MemoryMetricCallback
      memory_key:
        *memory_fake_probabilities: samples
        *memory_real_probabilities: samples_real
      prefix: "metrics/MS"
      metric: "ModeScore"

    frechet_inception_distance_conv:
      callback: MemoryMetricCallback
      memory_key:
        *memory_fake_features: samples_a
        *memory_real_features: samples_b
      prefix: "metrics/FID"
      metric: "FrechetInceptionDistance"

    # Metric-related callbacks [end]

  stage1:

    optimizer_params:
      _key_value: True
      generator:
        optimizer: Adam
        _model: [*generator_model_name]
        lr: 0.0001
        betas:
          - 0.5
          - 0.5
      discriminator:
        optimizer: Adam
        _model: [*critic_model_name]
        lr: 0.0001
        betas:
          - 0.5
          - 0.9
